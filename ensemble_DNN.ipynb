{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import tqdm\n",
    "from tensorflow import keras\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3579, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/aimlproject1/home/soft_cluster_comp.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# df3 = pd.read_csv(\"/home/aimlproject1/home/dfs/3.csv\")\n",
    "# s3 = df3\n",
    "# print(\"3\")\n",
    "# df4 = pd.read_csv(\"/home/aimlproject1/home/dfs/4.csv\")\n",
    "# s4 = df4\n",
    "# print(\"4\")\n",
    "# df5 = pd.read_csv(\"/home/aimlproject1/home/dfs/5.csv\")\n",
    "# s5 = df5\n",
    "# print(\"5\")\n",
    "# df6 = pd.read_csv(\"/home/aimlproject1/home/dfs/6.csv\")\n",
    "# s6 = df6\n",
    "# print(\"6\")\n",
    "# df7 = pd.read_csv(\"/home/aimlproject1/home/dfs/7.csv\")\n",
    "# s7 = df7\n",
    "# print(\"7\")\n",
    "df8 = pd.read_csv(\"/home/aimlproject1/home/dfs/8.csv\")\n",
    "s8 = df8\n",
    "print(\"8\")\n",
    "df9 = pd.read_csv(\"/home/aimlproject1/home/dfs/9.csv\")\n",
    "s9 = df9\n",
    "print(\"9\")\n",
    "# df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [\"d\", \"r\" ,\"df2\" , 'df3' , 'df4', 'df5' , 'df6' , \"df7\" , df8 , df9]\n",
    "def get_model():\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation=\"swish\")(investment_id_x)\n",
    "    investment_id_x = layers.BatchNormalization()(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation=\"swish\")(investment_id_x)\n",
    "    investment_id_x = layers.BatchNormalization()(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation=\"swish\")(investment_id_x)\n",
    "    investment_id_x = layers.BatchNormalization()(investment_id_x)\n",
    "    # \"swish\" tfa.activations.lisht\n",
    "    feature_x = layers.Dense(256, activation=\"swish\")(features_inputs)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.Dense(256, activation=\"swish\")(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.Dense(256, activation=\"swish\")(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([investment_id_x,feature_x])\n",
    "    x = layers.Dense(512, activation=\"swish\", kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(128, activation=\"swish\", kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(32, activation=\"swish\", kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_dir = [\n",
    " ['ensemle_0_model_1'],\n",
    "  ['ensemle_1_model_1'],\n",
    "   ['ensemle_2_model_1'],\n",
    "    ['ensemle_3_model_1'],\n",
    "     ['ensemle_4_model_1'],\n",
    "     ['ensemle_5_model_1'],\n",
    "     ['ensemle_6_model_1'],\n",
    "     ['ensemle_7_model_1'],\n",
    "     ['ensemle_8_model_1'],\n",
    "     ['ensemle_9_model_1']\n",
    "     ]\n",
    "checkpoints = []\n",
    "for i in  range(0,len(checkpoint_dir)):\n",
    "    checkpoints.append([])\n",
    "    for j in range(0,len(checkpoint_dir[i])): \n",
    "        checkpoint = ModelCheckpoint(filepath=checkpoint_dir[i][j], \n",
    "                            frequency = \"epoch\",\n",
    "                            save_weights_only = False,\n",
    "                             verbose= True)\n",
    "        checkpoints[i].append(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 14:09:54.209279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-18 14:09:54.275035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-18 14:09:54.275330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-18 14:09:54.279029: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-18 14:09:54.281363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-18 14:09:54.281690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-18 14:09:54.281960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-18 14:09:54.988102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-18 14:09:54.988403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-18 14:09:54.988628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-18 14:09:54.988868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 47229 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2022-04-18 14:09:55.420783: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "80794/80794 [==============================] - 1487s 18ms/step - loss: 0.8851 - mse: 0.8385 - mae: 0.6268 - mape: 101764.7891 - rmse: 0.9157\n",
      "\n",
      "Epoch 00001: saving model to ensemle_8_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 14:37:54.413122: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ensemle_8_model_1/assets\n",
      "Epoch 2/5\n",
      "80794/80794 [==============================] - 1497s 19ms/step - loss: 0.8319 - mse: 0.8291 - mae: 0.6229 - mape: 130241.8516 - rmse: 0.9106\n",
      "\n",
      "Epoch 00002: saving model to ensemle_8_model_1\n",
      "INFO:tensorflow:Assets written to: ensemle_8_model_1/assets\n",
      "Epoch 3/5\n",
      "80794/80794 [==============================] - 1485s 18ms/step - loss: 0.8281 - mse: 0.8268 - mae: 0.6220 - mape: 131258.9375 - rmse: 0.9093\n",
      "\n",
      "Epoch 00003: saving model to ensemle_8_model_1\n",
      "INFO:tensorflow:Assets written to: ensemle_8_model_1/assets\n",
      "Epoch 4/5\n",
      "80794/80794 [==============================] - 1484s 18ms/step - loss: 0.8256 - mse: 0.8248 - mae: 0.6213 - mape: 127783.7500 - rmse: 0.9082\n",
      "\n",
      "Epoch 00004: saving model to ensemle_8_model_1\n",
      "INFO:tensorflow:Assets written to: ensemle_8_model_1/assets\n",
      "Epoch 5/5\n",
      "80794/80794 [==============================] - 1483s 18ms/step - loss: 0.8236 - mse: 0.8232 - mae: 0.6208 - mape: 125591.7891 - rmse: 0.9073\n",
      "\n",
      "Epoch 00005: saving model to ensemle_8_model_1\n",
      "INFO:tensorflow:Assets written to: ensemle_8_model_1/assets\n",
      "9\n",
      "Epoch 1/5\n",
      "82004/82004 [==============================] - 1520s 19ms/step - loss: 0.8848 - mse: 0.8360 - mae: 0.6259 - mape: 118154.6641 - rmse: 0.9144\n",
      "\n",
      "Epoch 00001: saving model to ensemle_9_model_1\n",
      "INFO:tensorflow:Assets written to: ensemle_9_model_1/assets\n",
      "Epoch 2/5\n",
      "82004/82004 [==============================] - 1521s 19ms/step - loss: 0.8285 - mse: 0.8263 - mae: 0.6217 - mape: 134465.2500 - rmse: 0.9090\n",
      "\n",
      "Epoch 00002: saving model to ensemle_9_model_1\n",
      "INFO:tensorflow:Assets written to: ensemle_9_model_1/assets\n",
      "Epoch 3/5\n",
      "82004/82004 [==============================] - 1524s 19ms/step - loss: 0.8249 - mse: 0.8239 - mae: 0.6208 - mape: 135474.0000 - rmse: 0.9077\n",
      "\n",
      "Epoch 00003: saving model to ensemle_9_model_1\n",
      "INFO:tensorflow:Assets written to: ensemle_9_model_1/assets\n",
      "Epoch 4/5\n",
      "82004/82004 [==============================] - 1522s 19ms/step - loss: 0.8226 - mse: 0.8219 - mae: 0.6201 - mape: 142139.9531 - rmse: 0.9066\n",
      "\n",
      "Epoch 00004: saving model to ensemle_9_model_1\n",
      "INFO:tensorflow:Assets written to: ensemle_9_model_1/assets\n",
      "Epoch 5/5\n",
      "82004/82004 [==============================] - 1525s 19ms/step - loss: 0.8206 - mse: 0.8202 - mae: 0.6195 - mape: 134054.2969 - rmse: 0.9056\n",
      "\n",
      "Epoch 00005: saving model to ensemle_9_model_1\n",
      "INFO:tensorflow:Assets written to: ensemle_9_model_1/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(8,10):\n",
    "        print(i)\n",
    "        dd = df[i]\n",
    "        y = dd.pop(\"target\")\n",
    "        time_id = dd.pop(\"time_id\")\n",
    "        investment_id = dd.pop(\"investment_id\")\n",
    "        data = dd.loc[: , dd.columns != 'Unnamed: 0']\n",
    "\n",
    "        investment_ids = list(investment_id.unique())\n",
    "        investment_id_size = len(investment_ids) + 1\n",
    "        investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n",
    "\n",
    "        with tf.device(\"gpu\"):\n",
    "            investment_id_lookup_layer.adapt(investment_id)\n",
    "\n",
    "        model = get_model()\n",
    "        model.fit([investment_id, data],y, epochs=5  ,callbacks= [checkpoints[i][0]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
