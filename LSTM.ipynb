{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./target_lgb_filled.csv').drop('Unnamed: 0' , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3454</th>\n",
       "      <th>3455</th>\n",
       "      <th>3456</th>\n",
       "      <th>3457</th>\n",
       "      <th>3458</th>\n",
       "      <th>3459</th>\n",
       "      <th>3460</th>\n",
       "      <th>3461</th>\n",
       "      <th>3462</th>\n",
       "      <th>3463</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.121164</td>\n",
       "      <td>-0.300875</td>\n",
       "      <td>-0.231040</td>\n",
       "      <td>0.139263</td>\n",
       "      <td>-0.016268</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>-1.064780</td>\n",
       "      <td>-0.531940</td>\n",
       "      <td>1.505904</td>\n",
       "      <td>-0.260731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.211660</td>\n",
       "      <td>0.302557</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.114607</td>\n",
       "      <td>-0.392297</td>\n",
       "      <td>-0.877746</td>\n",
       "      <td>-0.284696</td>\n",
       "      <td>0.202003</td>\n",
       "      <td>0.009833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.094861</td>\n",
       "      <td>-0.917045</td>\n",
       "      <td>-0.472108</td>\n",
       "      <td>0.110934</td>\n",
       "      <td>0.099531</td>\n",
       "      <td>-0.147971</td>\n",
       "      <td>-0.372692</td>\n",
       "      <td>-0.105693</td>\n",
       "      <td>0.622500</td>\n",
       "      <td>-0.400038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068460</td>\n",
       "      <td>0.315893</td>\n",
       "      <td>-0.560079</td>\n",
       "      <td>0.250396</td>\n",
       "      <td>1.318857</td>\n",
       "      <td>-0.227782</td>\n",
       "      <td>-0.684049</td>\n",
       "      <td>-0.894825</td>\n",
       "      <td>-0.286612</td>\n",
       "      <td>0.115699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.143137</td>\n",
       "      <td>-0.480234</td>\n",
       "      <td>-0.661659</td>\n",
       "      <td>0.159401</td>\n",
       "      <td>0.035933</td>\n",
       "      <td>0.243674</td>\n",
       "      <td>0.318899</td>\n",
       "      <td>-0.260137</td>\n",
       "      <td>-0.610705</td>\n",
       "      <td>-1.030857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.230765</td>\n",
       "      <td>-0.305467</td>\n",
       "      <td>2.031675</td>\n",
       "      <td>-0.040981</td>\n",
       "      <td>-0.018971</td>\n",
       "      <td>-0.250995</td>\n",
       "      <td>-0.323800</td>\n",
       "      <td>0.300915</td>\n",
       "      <td>0.071693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.170280</td>\n",
       "      <td>-0.323562</td>\n",
       "      <td>-0.055215</td>\n",
       "      <td>0.228169</td>\n",
       "      <td>0.075363</td>\n",
       "      <td>1.816745</td>\n",
       "      <td>-0.711446</td>\n",
       "      <td>-0.640987</td>\n",
       "      <td>5.271096</td>\n",
       "      <td>-0.636719</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000747</td>\n",
       "      <td>0.258393</td>\n",
       "      <td>-0.730791</td>\n",
       "      <td>0.857357</td>\n",
       "      <td>0.386379</td>\n",
       "      <td>-0.708491</td>\n",
       "      <td>-0.165561</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>0.076417</td>\n",
       "      <td>0.046693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.149331</td>\n",
       "      <td>2.494479</td>\n",
       "      <td>0.341267</td>\n",
       "      <td>0.063783</td>\n",
       "      <td>0.121211</td>\n",
       "      <td>0.470476</td>\n",
       "      <td>-1.581650</td>\n",
       "      <td>-0.592970</td>\n",
       "      <td>0.288508</td>\n",
       "      <td>-0.435524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068460</td>\n",
       "      <td>0.247359</td>\n",
       "      <td>-0.656495</td>\n",
       "      <td>1.897659</td>\n",
       "      <td>-1.476258</td>\n",
       "      <td>-0.210125</td>\n",
       "      <td>-0.206145</td>\n",
       "      <td>0.126859</td>\n",
       "      <td>-0.387297</td>\n",
       "      <td>-0.181363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>0.579622</td>\n",
       "      <td>0.078338</td>\n",
       "      <td>-0.361899</td>\n",
       "      <td>1.624498</td>\n",
       "      <td>0.968079</td>\n",
       "      <td>-0.353440</td>\n",
       "      <td>-0.005973</td>\n",
       "      <td>-0.665880</td>\n",
       "      <td>-0.344785</td>\n",
       "      <td>-0.818674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074509</td>\n",
       "      <td>0.608381</td>\n",
       "      <td>-0.872156</td>\n",
       "      <td>-0.768846</td>\n",
       "      <td>-0.421168</td>\n",
       "      <td>0.045190</td>\n",
       "      <td>0.227717</td>\n",
       "      <td>-0.811419</td>\n",
       "      <td>0.292205</td>\n",
       "      <td>-0.049360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>0.064896</td>\n",
       "      <td>-0.290113</td>\n",
       "      <td>-0.688692</td>\n",
       "      <td>-0.070665</td>\n",
       "      <td>-0.514005</td>\n",
       "      <td>-0.095650</td>\n",
       "      <td>0.037676</td>\n",
       "      <td>0.095912</td>\n",
       "      <td>-0.161798</td>\n",
       "      <td>-0.489038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163917</td>\n",
       "      <td>0.666394</td>\n",
       "      <td>-0.918939</td>\n",
       "      <td>1.095260</td>\n",
       "      <td>-0.200416</td>\n",
       "      <td>-0.104977</td>\n",
       "      <td>-0.230372</td>\n",
       "      <td>-1.410046</td>\n",
       "      <td>0.651031</td>\n",
       "      <td>0.526346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>2.204664</td>\n",
       "      <td>-0.097136</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>1.532567</td>\n",
       "      <td>-0.207239</td>\n",
       "      <td>0.287350</td>\n",
       "      <td>0.139894</td>\n",
       "      <td>0.567659</td>\n",
       "      <td>-1.444997</td>\n",
       "      <td>0.313066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351041</td>\n",
       "      <td>0.511106</td>\n",
       "      <td>0.261462</td>\n",
       "      <td>-0.467051</td>\n",
       "      <td>0.019905</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.345404</td>\n",
       "      <td>-0.477728</td>\n",
       "      <td>-0.422950</td>\n",
       "      <td>1.492833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>1.037581</td>\n",
       "      <td>-0.080295</td>\n",
       "      <td>-0.082326</td>\n",
       "      <td>0.384600</td>\n",
       "      <td>0.318535</td>\n",
       "      <td>-0.187984</td>\n",
       "      <td>-0.007878</td>\n",
       "      <td>-0.586860</td>\n",
       "      <td>-0.371770</td>\n",
       "      <td>0.534930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.990797</td>\n",
       "      <td>-0.154512</td>\n",
       "      <td>-0.254095</td>\n",
       "      <td>-0.200416</td>\n",
       "      <td>-0.164939</td>\n",
       "      <td>0.644711</td>\n",
       "      <td>-0.119869</td>\n",
       "      <td>0.513739</td>\n",
       "      <td>-0.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>-2.631323</td>\n",
       "      <td>0.656833</td>\n",
       "      <td>0.032135</td>\n",
       "      <td>0.401314</td>\n",
       "      <td>-0.060477</td>\n",
       "      <td>0.026752</td>\n",
       "      <td>0.167933</td>\n",
       "      <td>-0.727649</td>\n",
       "      <td>-0.731196</td>\n",
       "      <td>-0.043783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337886</td>\n",
       "      <td>0.203811</td>\n",
       "      <td>-0.381538</td>\n",
       "      <td>0.351797</td>\n",
       "      <td>-0.200416</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>-0.223264</td>\n",
       "      <td>-0.559415</td>\n",
       "      <td>0.009599</td>\n",
       "      <td>1.212112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1211 rows × 3464 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.121164 -0.300875 -0.231040  0.139263 -0.016268  0.568807 -1.064780   \n",
       "1     0.094861 -0.917045 -0.472108  0.110934  0.099531 -0.147971 -0.372692   \n",
       "2     0.143137 -0.480234 -0.661659  0.159401  0.035933  0.243674  0.318899   \n",
       "3     0.170280 -0.323562 -0.055215  0.228169  0.075363  1.816745 -0.711446   \n",
       "4     0.149331  2.494479  0.341267  0.063783  0.121211  0.470476 -1.581650   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1206  0.579622  0.078338 -0.361899  1.624498  0.968079 -0.353440 -0.005973   \n",
       "1207  0.064896 -0.290113 -0.688692 -0.070665 -0.514005 -0.095650  0.037676   \n",
       "1208  2.204664 -0.097136  0.007041  1.532567 -0.207239  0.287350  0.139894   \n",
       "1209  1.037581 -0.080295 -0.082326  0.384600  0.318535 -0.187984 -0.007878   \n",
       "1210 -2.631323  0.656833  0.032135  0.401314 -0.060477  0.026752  0.167933   \n",
       "\n",
       "             7         8         9  ...      3454      3455      3456  \\\n",
       "0    -0.531940  1.505904 -0.260731  ...  0.016876  0.211660  0.302557   \n",
       "1    -0.105693  0.622500 -0.400038  ...  0.068460  0.315893 -0.560079   \n",
       "2    -0.260137 -0.610705 -1.030857  ...  0.016876  0.230765 -0.305467   \n",
       "3    -0.640987  5.271096 -0.636719  ... -0.000747  0.258393 -0.730791   \n",
       "4    -0.592970  0.288508 -0.435524  ...  0.068460  0.247359 -0.656495   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1206 -0.665880 -0.344785 -0.818674  ... -0.074509  0.608381 -0.872156   \n",
       "1207  0.095912 -0.161798 -0.489038  ...  0.163917  0.666394 -0.918939   \n",
       "1208  0.567659 -1.444997  0.313066  ...  0.351041  0.511106  0.261462   \n",
       "1209 -0.586860 -0.371770  0.534930  ... -0.029728  0.990797 -0.154512   \n",
       "1210 -0.727649 -0.731196 -0.043783  ...  0.337886  0.203811 -0.381538   \n",
       "\n",
       "          3457      3458      3459      3460      3461      3462      3463  \n",
       "0     0.003156  0.114607 -0.392297 -0.877746 -0.284696  0.202003  0.009833  \n",
       "1     0.250396  1.318857 -0.227782 -0.684049 -0.894825 -0.286612  0.115699  \n",
       "2     2.031675 -0.040981 -0.018971 -0.250995 -0.323800  0.300915  0.071693  \n",
       "3     0.857357  0.386379 -0.708491 -0.165561  0.836601  0.076417  0.046693  \n",
       "4     1.897659 -1.476258 -0.210125 -0.206145  0.126859 -0.387297 -0.181363  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1206 -0.768846 -0.421168  0.045190  0.227717 -0.811419  0.292205 -0.049360  \n",
       "1207  1.095260 -0.200416 -0.104977 -0.230372 -1.410046  0.651031  0.526346  \n",
       "1208 -0.467051  0.019905  0.577800  0.345404 -0.477728 -0.422950  1.492833  \n",
       "1209 -0.254095 -0.200416 -0.164939  0.644711 -0.119869  0.513739 -0.386294  \n",
       "1210  0.351797 -0.200416  0.033600 -0.223264 -0.559415  0.009599  1.212112  \n",
       "\n",
       "[1211 rows x 3464 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_ticker = df.iloc[: , 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "df_gen =  TimeseriesGenerator(np.array(df), np.array(df) , length= 5 , sampling_rate=1  ,batch_size = len(df)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "investment_ticker = df.iloc[: , 0 ]\n",
    "FORECAST_RANGE = 1  \n",
    "LOOK_BACK = 5 \n",
    "n_features= 1  \n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "validation = 0.1\n",
    "\n",
    "model_enc_dec = Sequential()\n",
    "model_enc_dec.add(LSTM(100, activation='relu', input_shape=(LOOK_BACK, n_features)))\n",
    "model_enc_dec.add(RepeatVector(FORECAST_RANGE))\n",
    "model_enc_dec.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model_enc_dec.add(TimeDistributed(Dense(n_features)))\n",
    "model_enc_dec.compile(optimizer='adam', loss='mse')\n",
    "plot_model(model=model_enc_dec, show_shapes=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1206, 5, 3464) (1206, 3464)\n",
      "[[ 0.12116425  0.09486069  0.14313655  0.17028032  0.14933099]\n",
      " [ 0.09486069  0.14313655  0.17028032  0.14933099  0.02324629]\n",
      " [ 0.14313655  0.17028032  0.14933099  0.02324629  0.10021492]\n",
      " ...\n",
      " [-2.6210136   0.12961906  1.1389745   0.57962227  0.06489602]\n",
      " [ 0.12961906  1.1389745   0.57962227  0.06489602  2.2046645 ]\n",
      " [ 1.1389745   0.57962227  0.06489602  2.2046645   1.0375807 ]]\n"
     ]
    }
   ],
   "source": [
    "x, y = df_gen[0]\n",
    "print(x.shape , y.shape)\n",
    "\n",
    "print(x[:,:,0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1701 - val_loss: 1.0909 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.1699 - val_loss: 1.0936 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.1701 - val_loss: 1.0951 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1700 - val_loss: 1.0900 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1698 - val_loss: 1.0930 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1700 - val_loss: 1.0912 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1699 - val_loss: 1.0938 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1699 - val_loss: 1.0923 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.1703 - val_loss: 1.0902 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1698 - val_loss: 1.0940 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.1702 - val_loss: 1.0948 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.7380 - val_loss: 0.4112 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7329 - val_loss: 0.4102 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7329 - val_loss: 0.4102 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7328 - val_loss: 0.4100 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7328 - val_loss: 0.4103 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.7330 - val_loss: 0.4102 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7329 - val_loss: 0.4107 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7332 - val_loss: 0.4101 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.7334 - val_loss: 0.4101 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7336 - val_loss: 0.4106 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7329 - val_loss: 0.4105 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.9225 - val_loss: 0.6313 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.9217 - val_loss: 0.6307 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9209 - val_loss: 0.6300 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9212 - val_loss: 0.6299 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9209 - val_loss: 0.6303 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.9208 - val_loss: 0.6302 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9203 - val_loss: 0.6300 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9206 - val_loss: 0.6301 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9207 - val_loss: 0.6300 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9203 - val_loss: 0.6300 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.9204 - val_loss: 0.6300 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.9040 - val_loss: 0.9648 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.9015 - val_loss: 0.9618 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9003 - val_loss: 0.9605 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8993 - val_loss: 0.9564 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.8983 - val_loss: 0.9558 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.8982 - val_loss: 0.9556 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8984 - val_loss: 0.9550 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8982 - val_loss: 0.9545 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8985 - val_loss: 0.9559 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8980 - val_loss: 0.9550 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8980 - val_loss: 0.9545 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.8979 - val_loss: 0.9543 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8980 - val_loss: 0.9539 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8980 - val_loss: 0.9542 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.0311 - val_loss: 0.4669 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.0311 - val_loss: 0.4670 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.0312 - val_loss: 0.4669 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0311 - val_loss: 0.4671 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0311 - val_loss: 0.4670 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0311 - val_loss: 0.4671 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0312 - val_loss: 0.4668 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0311 - val_loss: 0.4669 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0311 - val_loss: 0.4670 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0311 - val_loss: 0.4671 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0311 - val_loss: 0.4670 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7571 - val_loss: 0.8037 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7568 - val_loss: 0.8052 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7567 - val_loss: 0.8090 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7566 - val_loss: 0.8086 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.7564 - val_loss: 0.8130 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7564 - val_loss: 0.8138 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.7565 - val_loss: 0.8165 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7563 - val_loss: 0.8117 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7566 - val_loss: 0.8123 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7565 - val_loss: 0.8110 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.7566 - val_loss: 0.8133 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.6883 - val_loss: 0.0096 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6875 - val_loss: 0.0104 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.6866 - val_loss: 0.0114 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6865 - val_loss: 0.0128 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6864 - val_loss: 0.0124 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6861 - val_loss: 0.0131 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.6862 - val_loss: 0.0136 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6862 - val_loss: 0.0132 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6866 - val_loss: 0.0133 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6862 - val_loss: 0.0139 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6862 - val_loss: 0.0138 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 24ms/step - loss: 1.2389 - val_loss: 2.6006 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2349 - val_loss: 2.5924 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2331 - val_loss: 2.5893 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.2323 - val_loss: 2.5879 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2316 - val_loss: 2.5849 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2312 - val_loss: 2.5809 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2311 - val_loss: 2.5811 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2308 - val_loss: 2.5798 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2306 - val_loss: 2.5769 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2307 - val_loss: 2.5732 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2306 - val_loss: 2.5753 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.2307 - val_loss: 2.5770 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2310 - val_loss: 2.5731 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.2308 - val_loss: 2.5745 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2306 - val_loss: 2.5751 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.2307 - val_loss: 2.5767 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2305 - val_loss: 2.5758 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2306 - val_loss: 2.5759 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2305 - val_loss: 2.5773 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.2305 - val_loss: 2.5781 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.8258 - val_loss: 0.4158 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8260 - val_loss: 0.4158 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8260 - val_loss: 0.4156 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.8261 - val_loss: 0.4156 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.8260 - val_loss: 0.4157 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8258 - val_loss: 0.4157 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8259 - val_loss: 0.4157 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8258 - val_loss: 0.4157 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.8258 - val_loss: 0.4158 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.8263 - val_loss: 0.4156 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8258 - val_loss: 0.4157 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.8448 - val_loss: 0.6972 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.8420 - val_loss: 0.6930 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8406 - val_loss: 0.6907 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8398 - val_loss: 0.6894 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8391 - val_loss: 0.6872 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.8383 - val_loss: 0.6855 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8377 - val_loss: 0.6842 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8377 - val_loss: 0.6834 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8375 - val_loss: 0.6836 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8380 - val_loss: 0.6847 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.8376 - val_loss: 0.6836 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8374 - val_loss: 0.6828 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8374 - val_loss: 0.6822 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8373 - val_loss: 0.6822 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8373 - val_loss: 0.6822 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.8372 - val_loss: 0.6817 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.0698 - val_loss: 1.4211 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0688 - val_loss: 1.4220 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0679 - val_loss: 1.4241 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0676 - val_loss: 1.4247 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0676 - val_loss: 1.4243 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0676 - val_loss: 1.4249 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0675 - val_loss: 1.4247 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.0675 - val_loss: 1.4243 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.0675 - val_loss: 1.4251 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.0676 - val_loss: 1.4249 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0675 - val_loss: 1.4251 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.3519 - val_loss: 0.2420 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.3506 - val_loss: 0.2433 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.3504 - val_loss: 0.2439 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.3503 - val_loss: 0.2434 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.3504 - val_loss: 0.2442 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.3507 - val_loss: 0.2435 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.3504 - val_loss: 0.2438 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 27ms/step - loss: 0.3505 - val_loss: 0.2443 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 28ms/step - loss: 0.3504 - val_loss: 0.2441 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 28ms/step - loss: 0.3504 - val_loss: 0.2439 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.3504 - val_loss: 0.2441 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 0.9371 - val_loss: 0.4139 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.9364 - val_loss: 0.4141 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 24ms/step - loss: 0.9364 - val_loss: 0.4157 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 24ms/step - loss: 0.9364 - val_loss: 0.4165 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 24ms/step - loss: 0.9360 - val_loss: 0.4182 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.9360 - val_loss: 0.4193 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9362 - val_loss: 0.4180 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.9360 - val_loss: 0.4192 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9364 - val_loss: 0.4190 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9361 - val_loss: 0.4172 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9362 - val_loss: 0.4180 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.8027 - val_loss: 0.8783 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.8027 - val_loss: 0.8783 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.8027 - val_loss: 0.8784 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.8026 - val_loss: 0.8781 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.8026 - val_loss: 0.8781 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.8027 - val_loss: 0.8781 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 24ms/step - loss: 0.8026 - val_loss: 0.8782 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.8027 - val_loss: 0.8783 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.8026 - val_loss: 0.8783 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.8027 - val_loss: 0.8784 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.8026 - val_loss: 0.8784 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.0409 - val_loss: 0.6916 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0213 - val_loss: 0.6693 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0022 - val_loss: 0.6695 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0015 - val_loss: 0.6686 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0015 - val_loss: 0.6685 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0015 - val_loss: 0.6684 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0015 - val_loss: 0.6686 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0015 - val_loss: 0.6683 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0015 - val_loss: 0.6686 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.0015 - val_loss: 0.6688 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.0015 - val_loss: 0.6683 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0015 - val_loss: 0.6686 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.7621 - val_loss: 1.4729 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7501 - val_loss: 1.4688 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.7494 - val_loss: 1.4669 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7489 - val_loss: 1.4682 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7487 - val_loss: 1.4665 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.7484 - val_loss: 1.4658 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7484 - val_loss: 1.4656 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7483 - val_loss: 1.4653 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7484 - val_loss: 1.4647 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.7483 - val_loss: 1.4649 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7483 - val_loss: 1.4649 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.7483 - val_loss: 1.4649 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.7482 - val_loss: 1.4647 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 1.1624 - val_loss: 1.1656 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.1631 - val_loss: 1.1658 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 1.1626 - val_loss: 1.1655 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.1625 - val_loss: 1.1658 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.1623 - val_loss: 1.1657 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.1626 - val_loss: 1.1655 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.1631 - val_loss: 1.1654 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.1625 - val_loss: 1.1654 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.1626 - val_loss: 1.1656 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.1624 - val_loss: 1.1656 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.1624 - val_loss: 1.1656 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.6831 - val_loss: 1.8878 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6831 - val_loss: 1.8883 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6831 - val_loss: 1.8884 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.6832 - val_loss: 1.8881 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6832 - val_loss: 1.8879 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6831 - val_loss: 1.8884 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6831 - val_loss: 1.8884 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.6831 - val_loss: 1.8890 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6832 - val_loss: 1.8883 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.6832 - val_loss: 1.8886 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6831 - val_loss: 1.8881 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.4853 - val_loss: 0.0166 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.4842 - val_loss: 0.0163 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.4833 - val_loss: 0.0163 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.4830 - val_loss: 0.0167 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.4820 - val_loss: 0.0169 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.4819 - val_loss: 0.0171 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.4818 - val_loss: 0.0176 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.4818 - val_loss: 0.0178 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.4817 - val_loss: 0.0172 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.4820 - val_loss: 0.0172 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.4818 - val_loss: 0.0177 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.6381 - val_loss: 0.7565 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6368 - val_loss: 0.7552 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.6364 - val_loss: 0.7547 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.6361 - val_loss: 0.7547 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6362 - val_loss: 0.7546 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.6361 - val_loss: 0.7546 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6361 - val_loss: 0.7545 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.6361 - val_loss: 0.7545 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.6360 - val_loss: 0.7546 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6361 - val_loss: 0.7546 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6360 - val_loss: 0.7545 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.6995 - val_loss: 2.5779 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6929 - val_loss: 2.5671 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6890 - val_loss: 2.5658 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6887 - val_loss: 2.5659 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6901 - val_loss: 2.5666 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6882 - val_loss: 2.5670 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.6880 - val_loss: 2.5662 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.6885 - val_loss: 2.5666 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6880 - val_loss: 2.5658 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.6877 - val_loss: 2.5659 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.6878 - val_loss: 2.5670 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.6877 - val_loss: 2.5674 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.2313 - val_loss: 0.8121 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.2280 - val_loss: 0.8071 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2269 - val_loss: 0.8042 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2264 - val_loss: 0.8032 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2263 - val_loss: 0.8017 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2261 - val_loss: 0.8009 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.2260 - val_loss: 0.8005 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2260 - val_loss: 0.8006 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2260 - val_loss: 0.7996 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.2260 - val_loss: 0.7996 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2259 - val_loss: 0.7994 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2259 - val_loss: 0.7992 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2259 - val_loss: 0.7992 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2259 - val_loss: 0.7994 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.2259 - val_loss: 0.7982 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.3190 - val_loss: 0.4809 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.3189 - val_loss: 0.4812 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.3187 - val_loss: 0.4836 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.3186 - val_loss: 0.4837 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.3186 - val_loss: 0.4846 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.3186 - val_loss: 0.4845 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.3186 - val_loss: 0.4838 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.3185 - val_loss: 0.4834 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.3186 - val_loss: 0.4833 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.3186 - val_loss: 0.4823 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.3187 - val_loss: 0.4825 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.1864 - val_loss: 1.3716 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1779 - val_loss: 1.3966 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1746 - val_loss: 1.3900 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.1743 - val_loss: 1.4008 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1745 - val_loss: 1.3957 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1746 - val_loss: 1.3949 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.1746 - val_loss: 1.3997 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1744 - val_loss: 1.3953 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1746 - val_loss: 1.3935 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1746 - val_loss: 1.3927 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.1742 - val_loss: 1.3981 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.2532 - val_loss: 0.9797 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2425 - val_loss: 0.9841 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2424 - val_loss: 0.9827 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2425 - val_loss: 0.9825 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2425 - val_loss: 0.9829 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2425 - val_loss: 0.9838 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.2424 - val_loss: 0.9823 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2424 - val_loss: 0.9833 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.2425 - val_loss: 0.9830 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2423 - val_loss: 0.9827 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.2423 - val_loss: 0.9830 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.9960 - val_loss: 1.3181 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9955 - val_loss: 1.3184 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9953 - val_loss: 1.3190 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9954 - val_loss: 1.3184 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9952 - val_loss: 1.3191 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.9951 - val_loss: 1.3199 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.9952 - val_loss: 1.3211 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9952 - val_loss: 1.3210 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9954 - val_loss: 1.3207 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9952 - val_loss: 1.3202 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9951 - val_loss: 1.3208 - lr: 0.0010\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.9132 - val_loss: 0.1952 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9127 - val_loss: 0.1953 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9125 - val_loss: 0.1954 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9126 - val_loss: 0.1955 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.9122 - val_loss: 0.1957 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.9120 - val_loss: 0.1955 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.9121 - val_loss: 0.1956 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9126 - val_loss: 0.1962 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.9120 - val_loss: 0.1961 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "16/29 [===============>..............] - ETA: 0s - loss: 1.0589"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/aimlproject1/home/LSTM.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.16.192.52/home/aimlproject1/home/LSTM.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m y \u001b[39m=\u001b[39m y_0[:,i]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.16.192.52/home/aimlproject1/home/LSTM.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m X_train , X_test , y_train , y_test \u001b[39m=\u001b[39m x[:\u001b[39m1000\u001b[39m] , x[\u001b[39m1000\u001b[39m:], y[:\u001b[39m1000\u001b[39m] , y[\u001b[39m1000\u001b[39m: ]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.16.192.52/home/aimlproject1/home/LSTM.ipynb#ch0000005vscode-remote?line=7'>8</a>\u001b[0m history \u001b[39m=\u001b[39m model_enc_dec\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, validation_split\u001b[39m=\u001b[39;49mvalidation,callbacks\u001b[39m=\u001b[39;49m[early_stopping_callback, rlrop_callback])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.16.192.52/home/aimlproject1/home/LSTM.ipynb#ch0000005vscode-remote?line=8'>9</a>\u001b[0m yhat \u001b[39m=\u001b[39m model_enc_dec\u001b[39m.\u001b[39mpredict(X_test, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.192.52/home/aimlproject1/home/LSTM.ipynb#ch0000005vscode-remote?line=9'>10</a>\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(yhat)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///home/aimlproject1/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pearson = [] \n",
    "x_0 , y_0 = df_gen[0] \n",
    "\n",
    "for i in range(x_0.shape[2]) : \n",
    "    x  = x_0[:,:,i]\n",
    "    y = y_0[:,i]\n",
    "    X_train , X_test , y_train , y_test = x[:1000] , x[1000:], y[:1000] , y[1000:]\n",
    "    history = model_enc_dec.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation,callbacks=[early_stopping_callback, rlrop_callback])\n",
    "    yhat = model_enc_dec.predict(X_test, verbose=0)\n",
    "    pred = list(yhat)\n",
    "    hehe = np.squeeze(np.asarray(y_test))\n",
    "    haha = np.squeeze(np.asarray(pred))\n",
    "    result = np.all(haha == haha[0])\n",
    "    if result:\n",
    "        print('N')       \n",
    "    pearson_score = stats.pearsonr(haha, hehe)[0]\n",
    "    pearson.append(pearson_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07877054571739363\n"
     ]
    }
   ],
   "source": [
    "sum =  0 \n",
    "for i in pearson : \n",
    "    sum+=abs(i)\n",
    "print(sum / len(pearson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = x[:1000] , x[1000:], y[:1000] , y[1000: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    " monitor='val_loss',\n",
    " min_delta=0.005,\n",
    " patience=10,\n",
    " mode='min'\n",
    ")\n",
    "rlrop_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, mode='min', patience=3, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07989429274734065\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "yhat.reshape(1,-1)\n",
    "pred = list(yhat)\n",
    "hehe = np.squeeze(np.asarray(y_test))\n",
    "haha = np.squeeze(np.asarray(pred))\n",
    "result = np.all(haha == haha[0])\n",
    "if result:\n",
    "      print('N')       \n",
    "pearson_score = stats.pearsonr(haha, hehe)[0]\n",
    "print(pearson_score)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
